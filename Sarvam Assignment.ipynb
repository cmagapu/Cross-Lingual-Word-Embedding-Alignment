{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dab173d-c3a8-418b-9408-82c53bb36153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /opt/anaconda3/lib/python3.12/site-packages (0.9.3)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/anaconda3/lib/python3.12/site-packages (from fasttext) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from fasttext) (75.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from fasttext) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "# Install fasttext if not already installed\n",
    "!pip install fasttext\n",
    "\n",
    "import numpy as np\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d45a4-9c84-44b7-b29c-d7700a173472",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70413223-5e74-4da8-97a7-b4fa135ba4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading English fastText model...\n",
      "English model downloaded.\n",
      "Loading English model...\n",
      "English model loaded.\n",
      "English model download file deleted.\n",
      "Hindi model already exists.\n",
      "Loading Hindi model...\n",
      "Hindi model loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- English Model ---\n",
    "en_model_path = 'cc.en.300.bin'\n",
    "if not os.path.exists(en_model_path):\n",
    "    print(\"Downloading English fastText model...\")\n",
    "    fasttext.util.download_model('en', if_exists='ignore')\n",
    "    os.rename('cc.en.300.bin', en_model_path)\n",
    "    print(\"English model downloaded.\")\n",
    "else:\n",
    "    print(\"English model already exists.\")\n",
    "\n",
    "print(\"Loading English model...\")\n",
    "en_model = fasttext.load_model(en_model_path)\n",
    "print(\"English model loaded.\")\n",
    "\n",
    "# Deleting the downloaded file to free up disk space\n",
    "if os.path.exists(en_model_path):\n",
    "    os.remove(en_model_path)\n",
    "    print(\"English model download file deleted.\")\n",
    "\n",
    "# --- Hindi Model ---\n",
    "hi_model_path = 'cc.hi.300.bin'\n",
    "if not os.path.exists(hi_model_path):\n",
    "    print(\"Downloading Hindi fastText model...\")\n",
    "    fasttext.util.download_model('hi', if_exists='ignore')\n",
    "    os.rename('cc.hi.300.bin', hi_model_path)\n",
    "    print(\"Hindi model downloaded.\")\n",
    "else:\n",
    "    print(\"Hindi model already exists.\")\n",
    "\n",
    "print(\"Loading Hindi model...\")\n",
    "hi_model = fasttext.load_model(hi_model_path)\n",
    "print(\"Hindi model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dca1ef8-25d0-422a-8b1f-78e57459bd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'hello': [ 0.15757619  0.04378209 -0.00451272  0.06659314  0.07703468]...\n",
      "Vector for '‡§∞‡§æ‡§ú‡§ï‡•Å‡§Æ‡§æ‡§∞‡•Ä': [0.0275354  0.00600677 0.00590774 0.03287324 0.01008815]...\n"
     ]
    }
   ],
   "source": [
    "#Checking if we are able to get vectors from models\n",
    "english_word = \"hello\"\n",
    "hindi_word = \"‡§∞‡§æ‡§ú‡§ï‡•Å‡§Æ‡§æ‡§∞‡•Ä\"\n",
    "\n",
    "try:\n",
    "  english_vector = en_model.get_word_vector(english_word)\n",
    "  print(f\"Vector for '{english_word}': {english_vector[:5]}...\") # Print first 5 elements\n",
    "\n",
    "  hindi_vector = hi_model.get_word_vector(hindi_word)\n",
    "  print(f\"Vector for '{hindi_word}': {hindi_vector[:5]}...\") # Print first 5 elements\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Word '{e}' not found in the model vocabulary.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcea5a72-81bd-4aa5-a4d9-6ed2eb9509a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_translation_pairs(filepath):\n",
    "    \"\"\"\n",
    "    Extracts word translation pairs from a MUSE dictionary file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the dictionary file.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: List of (source_word, target_word) tuples.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                source_word, target_word = line.strip().split()\n",
    "                pairs.append((source_word, target_word))\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70aa7d4d-8cb1-449e-a575-8218fc47f8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  808k  100  808k    0     0  3530k      0 --:--:-- --:--:-- --:--:-- 3544k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  909k  100  909k    0     0  7153k      0 --:--:-- --:--:-- --:--:-- 7157k\n",
      "Extracted 38221 English-Hindi pairs.\n",
      "First 5 English-Hindi pairs: [('and', '‡§î‡§∞'), ('was', '‡§•‡§æ'), ('was', '‡§•‡•Ä'), ('for', '‡§≤‡§ø‡§Ø‡•á'), ('that', '‡§â‡§∏')]\n"
     ]
    }
   ],
   "source": [
    "# Download the bilingual dictionaries (you can comment these if already downloaded)\n",
    "#!curl -Lo hi-en.txt https://dl.fbaipublicfiles.com/arrival/dictionaries/hi-en.txt\n",
    "#!curl -Lo en-hi.txt https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
    "\n",
    "# Extract word pairs (here using English to Hindi pairs)\n",
    "en_hi_pairs = extract_translation_pairs('en-hi.txt')\n",
    "\n",
    "print(f\"Extracted {len(en_hi_pairs)} English-Hindi pairs.\")\n",
    "print(\"First 5 English-Hindi pairs:\", en_hi_pairs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f71a8834-78be-4276-ac4e-e51a3e7ad22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(src_embeddings, tgt_embeddings):\n",
    "    \"\"\"\n",
    "    Aligns source embeddings to target embeddings using Orthogonal Procrustes.\n",
    "\n",
    "    Args:\n",
    "        src_embeddings (np.ndarray): Source word embeddings (n x d).\n",
    "        tgt_embeddings (np.ndarray): Target word embeddings (n x d).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: (Aligned source embeddings, rotation matrix)\n",
    "    \"\"\"\n",
    "    # Find rotation matrix to map source embeddings to target space.\n",
    "    rotation, _ = orthogonal_procrustes(src_embeddings, tgt_embeddings)\n",
    "    aligned_src_embeddings = np.dot(src_embeddings, rotation)\n",
    "    return aligned_src_embeddings, rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4587c598-e1c0-40ee-8fdb-0e26771b19c5",
   "metadata": {},
   "source": [
    "# Embedding Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98927bab-f38a-4742-9692-f35f7a885df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment completed.\n"
     ]
    }
   ],
   "source": [
    "# Prepare embeddings for the bilingual lexicon (using English-Hindi pairs)\n",
    "english_embeddings = np.array([\n",
    "    en_model.get_word_vector(en_word) for en_word, hi_word in en_hi_pairs\n",
    "])\n",
    "hindi_embeddings = np.array([\n",
    "    hi_model.get_word_vector(hi_word) for en_word, hi_word in en_hi_pairs\n",
    "])\n",
    "\n",
    "# Align English embeddings into Hindi space.\n",
    "aligned_english_embeddings, rotation_matrix = align_embeddings(english_embeddings, hindi_embeddings)\n",
    "print(\"Alignment completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857c9a3-2bfc-497c-be8b-cef81f51d842",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "442eb311-f4c3-4ac3-a67a-2074fb2b5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    \"\"\"\n",
    "    Normalize word embeddings to unit vectors (for cosine similarity).\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): Embedding matrix of shape (n, d).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized embedding matrix.\n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return embeddings / norms\n",
    "\n",
    "def get_top_k_neighbors(query_vector, tgt_embeddings, tgt_words, k=5):\n",
    "    \"\"\"\n",
    "    Finds the top-k nearest neighbors using cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        query_vector (np.ndarray): Aligned embedding vector for a source word.\n",
    "        tgt_embeddings (np.ndarray): Normalized target embedding matrix (n x d).\n",
    "        tgt_words (List[str]): List of target words.\n",
    "        k (int): Number of neighbors to return.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of top-k nearest target words.\n",
    "    \"\"\"\n",
    "    query_norm = query_vector / np.linalg.norm(query_vector)\n",
    "    similarities = np.dot(tgt_embeddings, query_norm)\n",
    "    top_k_indices = np.argsort(-similarities)[:k]\n",
    "    return [tgt_words[i] for i in top_k_indices]\n",
    "\n",
    "def evaluate_translation(\n",
    "    translation_pairs,\n",
    "    en_model,\n",
    "    hi_model,\n",
    "    rotation_matrix,\n",
    "    top_k=[1, 5],\n",
    "    max_vocab_size=50000,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates translation accuracy using a test dictionary and aligned embeddings.\n",
    "\n",
    "    Args:\n",
    "        translation_pairs (List[Tuple[str, str]]): Test (English, Hindi) word pairs.\n",
    "        en_model (FastText model): Pre-trained English FastText model.\n",
    "        hi_model (FastText model): Pre-trained Hindi FastText model.\n",
    "        rotation_matrix (np.ndarray): Mapping from English to Hindi embedding space.\n",
    "        top_k (List[int]): Values of 'k' for Precision@k.\n",
    "        max_vocab_size (int): Limit the size of the Hindi vocabulary to search.\n",
    "        verbose (bool): Whether to print progress during evaluation.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, float]: Precision@k scores.\n",
    "    \"\"\"\n",
    "    correct_at_k = defaultdict(int)\n",
    "    total = 0\n",
    "\n",
    "    # Prepare Hindi vocab\n",
    "    hi_vocab = hi_model.get_words(include_freq=False)[:max_vocab_size]\n",
    "    hi_embeddings = np.array([hi_model.get_word_vector(w) for w in hi_vocab])\n",
    "    hi_embeddings_norm = hi_embeddings / np.linalg.norm(hi_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    for idx, (en_word, hi_word) in enumerate(translation_pairs):\n",
    "        try:\n",
    "            # Align English word into Hindi space\n",
    "            en_vec = en_model.get_word_vector(en_word)\n",
    "            aligned_vec = np.dot(en_vec, rotation_matrix)\n",
    "\n",
    "            # Normalize aligned vector\n",
    "            aligned_norm = aligned_vec / np.linalg.norm(aligned_vec)\n",
    "\n",
    "            # Cosine similarities = dot product (because all are normalized)\n",
    "            sims = np.dot(hi_embeddings_norm, aligned_norm)\n",
    "\n",
    "            # Top-k predictions\n",
    "            top_indices = np.argsort(-sims)[:max(top_k)]\n",
    "            top_predictions = [hi_vocab[i] for i in top_indices]\n",
    "\n",
    "            for k_val in top_k:\n",
    "                if hi_word in top_predictions[:k_val]:\n",
    "                    correct_at_k[k_val] += 1\n",
    "\n",
    "            total += 1\n",
    "\n",
    "            # Progress output\n",
    "            #if verbose and (idx + 1) % 100 == 0:\n",
    "                #print(f\"Evaluated {idx + 1} / {len(translation_pairs)} words...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            continue  # Skip OOV or errors\n",
    "\n",
    "    precision_scores = {k: correct_at_k[k] / total for k in top_k}\n",
    "    #if verbose:\n",
    "        #print(f\"\\n Evaluation complete on {total} word pairs.\")\n",
    "    return precision_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da00b2d0-68fa-4f62-aebd-c75c106f30a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 0.1659\n",
      "Precision@5: 0.3073\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using the English-Hindi test pairs\n",
    "scores = evaluate_translation(\n",
    "    translation_pairs=en_hi_pairs,\n",
    "    en_model=en_model,\n",
    "    hi_model=hi_model,\n",
    "    rotation_matrix=rotation_matrix,\n",
    "    top_k=[1, 5]\n",
    ")\n",
    "\n",
    "print(f\"Precision@1: {scores[1]:.4f}\")\n",
    "print(f\"Precision@5: {scores[5]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0038f9-2e80-4242-bbd6-3f6d68470704",
   "metadata": {},
   "source": [
    "**This means that**:\n",
    "\n",
    "In about 16.6% of test cases, the top predicted Hindi word was the correct translation.\n",
    "\n",
    "In about 30.7% of cases, the correct Hindi translation appeared within the top 5 predictions.\n",
    "\n",
    "These values may appear modest at first glance, but they are consistent with results reported in literature for distant language pairs like English‚ÄìHindi using unsupervised monolingual embeddings.\n",
    "\n",
    "The relatively low Precision@1 reflects the inherent difficulty of aligning independently trained word embeddings from these two typologically different languages using a linear transformation method like Procrustes.\n",
    "\n",
    "Precision@5 shows that the correct translation is often among the top candidates, indicating that the alignment is *semantically meaningful*, even if exact matches are not always ranked first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "415d4d65-2104-4a6c-a452-0813affa10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_study(pair_list, en_model, hi_model, test_pairs, sizes=[5000, 10000, 20000]):\n",
    "    \"\"\"\n",
    "    Runs an ablation study on bilingual lexicon sizes for supervised alignment.\n",
    "\n",
    "    Args:\n",
    "        pair_list (List[Tuple[str, str]]): Full bilingual lexicon (training pairs).\n",
    "        en_model (FastText model): Pretrained English FastText model.\n",
    "        hi_model (FastText model): Pretrained Hindi FastText model.\n",
    "        test_pairs (List[Tuple[str, str]]): Test pairs for evaluation.\n",
    "        sizes (List[int]): Lexicon sizes to try.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, Dict[int, float]]: Dictionary mapping lexicon size to precision scores.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for size in sizes:\n",
    "        print(f\"\\nüîß Running alignment with {size} training pairs...\")\n",
    "\n",
    "        # Limit training data\n",
    "        train_subset = pair_list[:size]\n",
    "\n",
    "        # Extract embeddings\n",
    "        try:\n",
    "            en_train_embeds = np.array([en_model.get_word_vector(e) for e, h in train_subset])\n",
    "            hi_train_embeds = np.array([hi_model.get_word_vector(h) for e, h in train_subset])\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping size {size} due to embedding error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Align\n",
    "        aligned_embeds, rotation = align_embeddings(en_train_embeds, hi_train_embeds)\n",
    "\n",
    "        # Evaluate\n",
    "        precision_scores = evaluate_translation(\n",
    "            translation_pairs=test_pairs,\n",
    "            en_model=en_model,\n",
    "            hi_model=hi_model,\n",
    "            rotation_matrix=rotation,\n",
    "            top_k=[1, 5]\n",
    "        )\n",
    "\n",
    "        print(f\"Lexicon size {size}: P@1 = {precision_scores[1]:.4f}, P@5 = {precision_scores[5]:.4f}\")\n",
    "        results[size] = precision_scores\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_ablation_results(results_dict):\n",
    "    sizes = sorted(results_dict.keys())\n",
    "    p1_scores = [results_dict[s][1] for s in sizes]\n",
    "    p5_scores = [results_dict[s][5] for s in sizes]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(sizes, p1_scores, marker='o', label='Precision@1')\n",
    "    plt.plot(sizes, p5_scores, marker='s', label='Precision@5')\n",
    "    plt.title(\"Ablation Study: Impact of Lexicon Size on Alignment Quality\")\n",
    "    plt.xlabel(\"Lexicon Size\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"ablation_study.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247bd07b-d49f-408d-b4fa-343d74dc5570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Running alignment with 5000 training pairs...\n",
      "Lexicon size 5000: P@1 = 0.1259, P@5 = 0.2534\n",
      "\n",
      "üîß Running alignment with 10000 training pairs...\n",
      "Lexicon size 10000: P@1 = 0.1483, P@5 = 0.2867\n",
      "\n",
      "üîß Running alignment with 20000 training pairs...\n"
     ]
    }
   ],
   "source": [
    "ablation_results = run_ablation_study(\n",
    "    pair_list=en_hi_pairs,\n",
    "    en_model=en_model,\n",
    "    hi_model=hi_model,\n",
    "    test_pairs=en_hi_pairs,  # or use a dedicated test set\n",
    "    sizes=[5000, 10000, 20000]\n",
    ")\n",
    "\n",
    "# Print results clearly\n",
    "for size, scores in ablation_results.items():\n",
    "    print(f\"Lexicon size {size} ‚Üí P@1: {scores[1]:.4f}, P@5: {scores[5]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbada800-dbdf-468b-b153-ec0c84b602ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and store results\n",
    "ablation_results = run_ablation_study(\n",
    "    pair_list=en_hi_pairs,\n",
    "    en_model=en_model,\n",
    "    hi_model=hi_model,\n",
    "    test_pairs=en_hi_pairs,  # You can also use a separate test set if needed\n",
    "    sizes=[5000, 10000, 20000]\n",
    ")\n",
    "\n",
    "for size, scores in ablation_results.items():\n",
    "    print(f\"Lexicon size {size} ‚Üí Precision@1: {scores[1]:.4f}, Precision@5: {scores[5]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fbf2ba-b390-442c-9a50-2fbfcc3336f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plot_ablation_results(ablation_results)\n",
    "plt.savefig(\"ablation_study.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d2986c-f19e-4ca8-8bff-eb99cbad49b3",
   "metadata": {},
   "source": [
    "Increasing the size of the training lexicon consistently improves both Precision@1 and Precision@5.\n",
    "\n",
    "The most significant gain occurs when moving from 5k to 10k pairs, suggesting that a moderately sized bilingual dictionary already captures much of the necessary structure for alignment.\n",
    "\n",
    "The improvements begin to plateau between 10k and 20k, indicating diminishing returns for larger dictionaries.\n",
    "\n",
    "This trend highlights the practical trade-off between annotation cost (or lexicon availability) and alignment quality ‚Äî and motivates the use of unsupervised methods when large dictionaries aren't available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e245192-e545-43a2-ab77-3df59f659cb4",
   "metadata": {},
   "source": [
    "# Unsupervised Alignment (extra credit) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85291ba4-4d46-41d0-a550-f5275adc9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set manual seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Mapping W: learnable orthogonal matrix (initialized as identity)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.W = nn.Linear(dim, dim, bias=False)\n",
    "        self.W.weight.data.copy_(torch.eye(dim))  # Start as identity\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W(x)\n",
    "\n",
    "# Discriminator D: binary classifier to distinguish real/fake\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(dim, 2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(2048, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede0ff5-a18d-4aff-af76-b802e72c408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_normalize_embeddings(ft_model, max_vocab=50000):\n",
    "    \"\"\"\n",
    "    Loads and normalizes FastText embeddings from a pretrained model.\n",
    "\n",
    "    Args:\n",
    "        ft_model: A loaded fastText model (English or Hindi).\n",
    "        max_vocab (int): Maximum number of most frequent words to load.\n",
    "\n",
    "    Returns:\n",
    "        embeddings (torch.Tensor): Normalized embeddings (n_words x dim).\n",
    "        vocab (List[str]): Corresponding word list.\n",
    "    \"\"\"\n",
    "    vocab = ft_model.get_words(include_freq=False)[:max_vocab]\n",
    "    embeddings = np.array([ft_model.get_word_vector(w) for w in vocab])\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized = embeddings / norms\n",
    "    return torch.tensor(normalized, dtype=torch.float32).to(device), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1704e08-7a9f-420c-a5c9-e31465567a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and normalize top 50k English and Hindi embeddings\n",
    "en_embeddings, en_vocab = load_and_normalize_embeddings(en_model, max_vocab=50000)\n",
    "hi_embeddings, hi_vocab = load_and_normalize_embeddings(hi_model, max_vocab=50000)\n",
    "\n",
    "print(\"English embedding shape:\", en_embeddings.shape)\n",
    "print(\"Hindi embedding shape:\", hi_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031c5932-cf97-4511-a1fe-7b2e35422154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_train_loop(\n",
    "    src_embeddings, tgt_embeddings,\n",
    "    generator, discriminator,\n",
    "    num_epochs=10, batch_size=128,\n",
    "    d_steps=5, lr_g=0.1, lr_d=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs adversarial training to learn a mapping from source to target space.\n",
    "\n",
    "    Args:\n",
    "        src_embeddings (torch.Tensor): English embeddings (n x d).\n",
    "        tgt_embeddings (torch.Tensor): Hindi embeddings (n x d).\n",
    "        generator (nn.Module): The mapping (W).\n",
    "        discriminator (nn.Module): The binary classifier (D).\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for both D and G.\n",
    "        d_steps (int): Number of discriminator steps per generator step.\n",
    "        lr_g (float): Learning rate for generator.\n",
    "        lr_d (float): Learning rate for discriminator.\n",
    "\n",
    "    Returns:\n",
    "        generator (nn.Module): Trained generator (mapping W).\n",
    "    \"\"\"\n",
    "    g_opt = optim.SGD(generator.parameters(), lr=lr_g)\n",
    "    d_opt = optim.SGD(discriminator.parameters(), lr=lr_d)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    n = src_embeddings.shape[0]\n",
    "    d = src_embeddings.shape[1]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        perm = torch.randperm(n)\n",
    "\n",
    "        for i in range(0, n, batch_size):\n",
    "            # Sample real Hindi (target) embeddings\n",
    "            batch_indices = perm[i:i+batch_size]\n",
    "            real_tgt = tgt_embeddings[batch_indices]\n",
    "\n",
    "            # Sample matching English embeddings and map to Hindi space\n",
    "            real_src = src_embeddings[batch_indices]\n",
    "            fake_tgt = generator(real_src).detach()\n",
    "\n",
    "            # === Step 1: Train Discriminator ===\n",
    "            for _ in range(d_steps):\n",
    "                # Inputs and labels\n",
    "                d_real_preds = discriminator(real_tgt)\n",
    "                d_fake_preds = discriminator(fake_tgt)\n",
    "\n",
    "                d_loss_real = loss_fn(d_real_preds, torch.ones_like(d_real_preds))\n",
    "                d_loss_fake = loss_fn(d_fake_preds, torch.zeros_like(d_fake_preds))\n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "                d_opt.zero_grad()\n",
    "                d_loss.backward()\n",
    "                d_opt.step()\n",
    "\n",
    "            # === Step 2: Train Generator (to fool D) ===\n",
    "            fake_tgt = generator(real_src)\n",
    "            preds = discriminator(fake_tgt)\n",
    "            g_loss = loss_fn(preds, torch.ones_like(preds))  # want D to think fake is real\n",
    "\n",
    "            g_opt.zero_grad()\n",
    "            g_loss.backward()\n",
    "            g_opt.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    return generator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
